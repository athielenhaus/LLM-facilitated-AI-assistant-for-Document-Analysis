{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34d22cd",
   "metadata": {},
   "source": [
    "# Parsing documents for higher education Quality Assurance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659e22f",
   "metadata": {},
   "source": [
    "### Brief Introduction to Higher Education Quality Assurance in Germany\n",
    "\n",
    "Accreditation is a requirement of the German higher education system by which study programs and higher education institutions (HEIs) regularly undergo quality checks through external specialised agencies:\n",
    "\n",
    "* the objective is to to ensure conformity with state-level higher education regulations. \n",
    "* these regulations include formal, objective requirements, pertaining for example to:\n",
    "\n",
    "    - the length of degree programs\n",
    "    - the degree types which can be awarded\n",
    "    - the structure of the degree programs\n",
    "    - the proper usage of the European Credit Transfer System (ECTS)\n",
    "    - transparency of information\n",
    "    - cooperation with other institutions\n",
    "    \n",
    "An example of one such regulation:\n",
    "\n",
    "    \"The standard periods of study for full-time study are six, seven or eight semesters for bachelor's programs and four, three or two semesters for master's programs. For bachelor's degree programs, the standard period of study for full-time study is at least three years. For consecutive degree programs, the total standard period of full-time study is five years (ten semesters).\" (Source: Musterrechtsverordnung, Stiftung Akkreditierungsrat)\n",
    "\n",
    "#### Current quality assurance procedure\n",
    "To determine whether or not these formal criteria are met, auditors review the official documentation provided by the higher education institution. This documentation typically includes the syllabus, the examination regulations (_Prüfungsordnung_), the Module Catalog (_Modulhandbuch_) and a variety of other documents. Subsequently, auditors summarize their findings in the official Accreditation Report, which is submitted to the German Accreditation Board.\n",
    "\n",
    "Essentially, this part of the procedure is a simple search and check exercise which, however, due to the often vast amounts of documentation provided, can be tedious to complete. Formal HEI documentation is seldomly structured in a manner conducive to auditing, and can significantly differ in terms of structure and content from one institution to the next. \n",
    "\n",
    "#### Project Objective\n",
    "\n",
    "The goal of this project is therefore to parse formal HEI documents, check them for content relevant to the official formal criteria, and to - with the help of a Large Language Model (LLM) - summarize the findings in a manner so that they can be copy-pasted into the Accreditation Report. At the same time, we want to determine whether this is a viable approach in terms of costs.\n",
    "\n",
    "While there are presumably multiple use-cases for LLMs in the context of accreditation, the one described above may be among the simplest ones to achieve, also because data samples in the form of formal documents published on German HEI websites are readily available. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff7c94",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "the tools used will include:\n",
    "\n",
    "* a PDF reader: almost all documentation for accreditation procedures is submitted in PDF format\n",
    "* Langchain: a library which simplifies working with LLMs. It includes:\n",
    "    - OpenAI Embeddings: fast embeddings API which charges a small amount per submitted token.\n",
    "    - Huggingface Embeddings: for comparison we are also testing this free and high quality embeddings tool, although it takes up to 30x longer when run on a regular CPU.\n",
    "* OpenAI GPT API: for accessing the GPT LLM \n",
    "* tiktoken: for converting text to tokens in order to estimate costs prior to embedding.\n",
    "* Streamlit: a library for creating a simple user interface.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9c2a3",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1819e0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import openai\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "\n",
    "# import langchain tools\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS                # Facebook AI Similarity Search - vectors are stored on machine - will be deleted once application is closed\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA, LLMChain, HypotheticalDocumentEmbedder\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8adc7d",
   "metadata": {},
   "source": [
    "## Create functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1178d6",
   "metadata": {},
   "source": [
    "### PDF text extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309250ec",
   "metadata": {},
   "source": [
    "#### PDFMiner \n",
    "docs: https://github.com/euske/pdfminer/blob/master/pdfminer/pdfpage.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8772dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PDF Miner\n",
    "import io \n",
    "from pdfminer.converter import TextConverter \n",
    "from pdfminer.pdfinterp import PDFPageInterpreter \n",
    "from pdfminer.pdfinterp import PDFResourceManager \n",
    "from pdfminer.pdfpage import PDFPage \n",
    "\n",
    "\n",
    "def extract_text_by_page(pdf_path): \n",
    "\n",
    "    with open(pdf_path, 'rb') as fh: \n",
    "        \n",
    "        for page in PDFPage.get_pages(fh, \n",
    "                                    caching=True, \n",
    "                                    check_extractable=True): \n",
    "            \n",
    "            resource_manager = PDFResourceManager() \n",
    "\n",
    "            fake_file_handle = io.StringIO() \n",
    "            \n",
    "            converter = TextConverter(resource_manager, \n",
    "                                    fake_file_handle) \n",
    "            \n",
    "            page_interpreter = PDFPageInterpreter(resource_manager, \n",
    "                                                converter) \n",
    "            \n",
    "            page_interpreter.process_page(page) \n",
    "            text = fake_file_handle.getvalue() \n",
    "            \n",
    "            yield text \n",
    "            \n",
    "            # close open handles \n",
    "            converter.close() \n",
    "            fake_file_handle.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17002fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text extracted via PDFminer still has hyphens from line-breaks, therefore we create another function\n",
    "\n",
    "import re\n",
    "\n",
    "def replace_hyphens(text):\n",
    "    pattern = r'([a-z])-([a-z])'  # Pattern to match 'lowercase letter - lowercase letter'\n",
    "    replacement = r'\\1\\2'         # Replacement pattern is equivalent to ''\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Iterate over the matches and replace the hyphen-separated lowercase letters\n",
    "    for match in matches:\n",
    "        text = text.replace(f'{match[0]}-{match[1]}', f'{match[0]}{match[1]}')\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045c29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks\n",
    "def get_text_chunks(text):\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        #separator= \"\\n\",\n",
    "        chunk_size= 1000,\n",
    "        chunk_overlap= 200,\n",
    "        length_function= len\n",
    "    )\n",
    "    chunks= text_splitter.split_text(text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c93147c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tokens which will be submitted for embedding as well as price\n",
    "def get_nr_of_tokens_and_price(chunks, price_per_1k_tokens):\n",
    "    \n",
    "    '''takes as arguments chunks created via previous function as well as price which can be researched on OpenAI website\n",
    "    (https://openai.com/pricing)'''\n",
    "    \n",
    "    nr_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        enc = tiktoken.get_encoding(\"p50k_base\")\n",
    "        chunk_tokens = enc.encode(chunk)\n",
    "        nr_tokens += len(chunk_tokens)\n",
    "        \n",
    "    price = round((nr_tokens / 1000) * price_per_1k_tokens, 4)\n",
    "        \n",
    "    return nr_tokens, price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0df6fd",
   "metadata": {},
   "source": [
    "### Functions for creating Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56cd818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create normal and Hypothetical Document Embedding (HyDE) vector stores\n",
    "def get_vectorstore(text_chunks, create_hyde_store= True):\n",
    "    \n",
    "    # setup normal vector store\n",
    "#     if response == '1':\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_texts(text_chunks, embedding= embeddings) \n",
    "        \n",
    "        # setup HyDE vector store\n",
    "    if create_hyde_store:\n",
    "        llm = OpenAI(temperature= 0)\n",
    "        embeddings_hyde = HypotheticalDocumentEmbedder.from_llm(llm, embeddings, \"web_search\")\n",
    "        vectorstore_hyde = FAISS.from_texts(text_chunks, embedding= embeddings_hyde) \n",
    "    \n",
    "    print('EMBEDDING COMPLETED!')\n",
    "    \n",
    "    return vectorstore, vectorstore_hyde\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2320b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings_with_user():\n",
    "    \n",
    "    # get price estimate\n",
    "    price_per_1k_tokens = 0.0001\n",
    "    nr_tokens, price = get_nr_of_tokens_and_price(text_chunks, price_per_1k_tokens)\n",
    "\n",
    "    # communicate price of embedding and check with user if they want to proceed\n",
    "    response = input(\n",
    "        f'''The submitted text files have a length of {len(cleaned_text)} characters equivalent to {nr_tokens} tokens. \n",
    "\n",
    "    We can quickly embed these tokens for a total price of US$ {price} (rounded to 4 decimals). \n",
    "    Type '1' if you want to proceed with paid embedding. \n",
    "    Typing anything else will result in no action.\\n''')\n",
    "\n",
    "    if response == '1':\n",
    "        create_store = True\n",
    "        hyde_response = input(\n",
    "    '''\\nType \"1\" again if you also want a HyDE vector store (NOTE: this will double the embedding cost)\\nTyping anything else will result in only a normal vector store being created\\n\\n''')\n",
    "\n",
    "        if hyde_response == '1':\n",
    "            create_hyde = True\n",
    "        else:\n",
    "            create_hyde = False\n",
    "    else:\n",
    "        create_store = False\n",
    "    \n",
    "    return create_store, create_hyde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1b1a9",
   "metadata": {},
   "source": [
    "### Functions for Key Word Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64ff8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_key_words(text, word):\n",
    "    word_count = text.lower().count(word)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d500d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preferred_terms(text, term_dict= {\n",
    "    \"fin_proj_terms\" : [\"abschlussarbeit\", \"bachelorarbeit\", \"masterarbeit\"],\n",
    "    \"creditpoint_terms\" : [\"kreditpunkte\", \"leistungspunkte\", \"ects-punkte\", \"ects punkte\", \" lp \"],\n",
    "    \"degree_level\": [\"bachelorstudiengang\", \"masterstudiengang\"],\n",
    "    \"degree_designation\" : [\"hochschulgrad\", \"abschlussgrad\", \"akademische Grad\"]}\n",
    "                          ):\n",
    "    \n",
    "    preferred_terms_dict = {}\n",
    "\n",
    "    for x in term_dict:\n",
    "        # get list of terms for category, e.g. \"fin_proj_terms\"\n",
    "        term_list = term_dict[x]\n",
    "\n",
    "        # get word count for each term in the list and add to new dict\n",
    "        term_count_dict = {word:count_key_words(text, word) for word in term_list}\n",
    "\n",
    "        # get term with highest occurence\n",
    "        max_key = max(term_count_dict, key= term_count_dict.get)\n",
    "    #     max_value = max(term_count_dict.values())\n",
    "    \n",
    "        preferred_terms_dict[x] = max_key\n",
    "    #     preferred_terms_dict[x] = {max_key: max_value}\n",
    "\n",
    "    return preferred_terms_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ec3dd",
   "metadata": {},
   "source": [
    "## PDF Upload and Text Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a30b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "file_path = '/Users/Arne/Downloads/Prüfungsordnungen/KIT_Ba_Informatik.pdf'\n",
    "\n",
    "# extract individual pages\n",
    "doc = extract_text_by_page(file_path)\n",
    "pages = [page for page in doc]\n",
    "\n",
    "# besides replacing hyphens we replace a string automatically added by PDFminer\n",
    "cleaned_pages = [replace_hyphens(page).replace(\"\\x0c\", \"\") for page in pages]\n",
    "\n",
    "cleaned_text = ''.join(cleaned_pages)\n",
    "\n",
    "# get text chunks\n",
    "text_chunks = get_text_chunks(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1af173",
   "metadata": {},
   "source": [
    "## Setup Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886820a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The submitted text files have a length of 51608 characters equivalent to 25220 tokens. \n",
      "\n",
      "    We can quickly embed these tokens for a total price of US$ 0.0025 (rounded to 4 decimals). \n",
      "    Type '1' if you want to proceed with paid embedding. \n",
      "    Typing anything else will result in no action.\n",
      "1\n",
      "\n",
      "Type \"1\" again if you also want a HyDE vector store (NOTE: this will double the embedding cost)\n",
      "Typing anything else will result in only a normal vector store being created\n",
      "\n",
      "1\n",
      "Returning  True True\n",
      "EMBEDDING COMPLETED!\n"
     ]
    }
   ],
   "source": [
    "create_store, create_hyde = check_embeddings_with_user()\n",
    "print(\"Returning \", create_store, create_hyde)\n",
    "\n",
    "if create_store:\n",
    "    vector_store, vector_store_hyde = get_vectorstore(text_chunks, create_hyde_store= create_hyde)\n",
    "else:\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c99e92",
   "metadata": {},
   "source": [
    "## Identify key terms\n",
    "\n",
    "The document retrieval chain is sensitive to terminology - small differences in the submitted prompts, for example using the abbreviation \"LP\" for \"Leistungspunkte\" (or in English: using \"CP\" to refer to \"credit points\"), can generate very different results. We therefore first parse the text to identify the HEI-specific terminology for areas that are key to the analysis, in order to optimize the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aaacb130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fin_proj_terms': 'bachelorarbeit',\n",
       " 'creditpoint_terms': 'leistungspunkt',\n",
       " 'degree_level': 'bachelorstudiengang',\n",
       " 'degree_designation': 'hochschulgrad'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following dict can be edited\n",
    "term_dict = {\"fin_proj_terms\" : [\"abschlussarbeit\", \"bachelorarbeit\", \"masterarbeit\", \"doktorarbeit\"],\n",
    "             \"creditpoint_terms\" : [\"kreditpunkt\", \"leistungspunkt\", \"ects-punkt\", \"ects punkt\", \" lp \"],\n",
    "             \"degree_level\": [\"bachelorstudiengang\", \"masterstudiengang\", \"phd\"],\n",
    "             \"degree_designation\" : [\"hochschulgrad\", \"abschlussgrad\", \"akademische Grad\"]\n",
    "            }\n",
    "\n",
    "preferred_terms = get_preferred_terms(cleaned_text, term_dict) # term_dict is an optional argument (default is included in function)\n",
    "\n",
    "preferred_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b67f5",
   "metadata": {},
   "source": [
    "### Adjust prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bachelor\" in preferred_terms['degree_level']:\n",
    "    degree= \"bachelor\"\n",
    "elif \"master\" in preferred_terms['degree_level']:\n",
    "    degree= \"master\"\n",
    "else:\n",
    "    degree= \"\"\n",
    "\n",
    "prompts = [\"Was ist die Regelstudienzeit des Studiengangs? Welche Ausnahmen gibt es?\", \n",
    "           f\"Wieviele {preferred_terms['creditpoint_terms']}e umfasst der Studiengang?\",\n",
    "           f'''Was ist der Umfang der {preferred_terms['fin_proj_terms']}, bzw. wieviele {preferred_terms['creditpoint_terms']}e umfasst die {preferred_terms['fin_proj_terms']}? welchen umfang hat das modul '{preferred_terms[\"fin_proj_terms\"]}'?''',\n",
    "           f\"Welcher akademische Grad wird verliehen? (z.B. '{degree} of Science', '{degree} of Arts' oder '{degree} of Engineering')\",\n",
    "           \"Wie sind die Zugangsvoraussetzungen für den Studiengang?\",\n",
    "           f\"Wieviele Arbeitsstunden sind in einem {preferred_terms['creditpoint_terms']} enthalten?\",\n",
    "           \"In wiefern werden ein Diploma Supplement und ein Zeugnis und ein Transcript of Records ausgestellt?\"\n",
    "          ]\n",
    "\n",
    "if degree == 'master':\n",
    "    prompts.append(\"Ist der Masterstudiengang konsekutiv oder weiterbildend?\")\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b97010",
   "metadata": {},
   "source": [
    "## Setup Retrieval Chains\n",
    "This sets up a chain which can take user queries as an input. The queries are embedded and similar documents are retrieved from the vector store. The LLM then formulates an answer based on the retrieved text chunks.\n",
    "\n",
    "In the case of the HyDE chain, the user query and the LLM are used to create a Hypothetical answer to the query. For example:\n",
    "\n",
    "- User query: \"What did the president say about relations with Indonesia?\"\n",
    "- Hypothetical answer generated by LLM: \"The president said that U.S. relations with Indonesia have grown closer and closer.\"\n",
    "\n",
    "This hypothetical answer is then embedded and submitted to the vector store INSTEAD OF (or in addition to?) THE USER QUERY in order to retrieve relevant documents. This can result in a much better match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df1f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature= 0.0)   # initialize LLM model\n",
    "turbo_llm = ChatOpenAI(temperature= 0.0,\n",
    "                       model_name='gpt-3.5-turbo')\n",
    "\n",
    "                       \n",
    "'''Notes on from_llm method: takes llm, prompt (see prompt template) and any kwargs as arguments'''\n",
    "\n",
    "retrieval_chain = RetrievalQA.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    #memory = memory,\n",
    "    return_source_documents= True\n",
    ")\n",
    "\n",
    "\n",
    "# create retrieval chain with HyDE vectorstore\n",
    "retrieval_chain_hyde = RetrievalQA.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = vector_store_hyde.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents= True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ee577",
   "metadata": {},
   "source": [
    "### Comparison of queries\n",
    "\n",
    "The sample questions and answers below provide an indication of the sensitivity of the model to terminology used in the query. The relevant text in the document reads: \"Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten.\" \n",
    "\n",
    "While the first query results in a correct answer, neither the second nor third query are unable to detect it. It should be noted that the first query contains 4 of the words that are contained in the relevant string, in the same order (...der Umfang der Bachelorarbeit)\n",
    "\n",
    "This indicates that, when searching for relevant documents, query wording is extremely important. In fact, even capitalization appears to play a role, as it changes the response.\n",
    "\n",
    "Possible measures to improve performance:\n",
    "- use longer questions which through the usage of more words may result in embeddings with a higher proximity\n",
    "- lowercase all words before embedding\n",
    "- include metadata in both the documents and the queries\n",
    "- use different embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b629417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of 3 queries with slight differences in wording\n",
    "queries = ['was ist der umfang der bachelorarbeit?',\n",
    "           'wieviele leistungspunkte umfängt die bachelorarbeit?',\n",
    "           'wieviele leistungspunkten entspricht die bachelorarbeit?',\n",
    "          \"Was ist der Umfang der bachelorarbeit, bzw. wieviele leistungspunkte umfasst die bachelorarbeit? welchen umfang hat das modul 'bachelorarbeit'?\"]\n",
    "\n",
    "# for query in queries:\n",
    "#     result = retrieval_chain({\"query\": query})\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "021083ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': 'was ist der umfang der bachelorarbeit?',\n",
       "  'result': ' Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten.',\n",
       "  'source_documents': [Document(page_content='nach wissenschaftlichen Methoden zu bearbeiten. 2Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten. 3Die maximale Bearbeitungsdauer beträgt vier Monate. 4Thema und Aufgabenstellung sind an den vorgesehenen Umfang anzupassen. 5Der Prüfungsausschuss legt fest, in welchen Sprachen die Bachelorarbeit geschrieben werden kann. 6Auf Antrag des Studierenden kann der/die Prüfende genehmigen, dass die Bachelorarbeit in einer anderen Sprache als Deutsch geschrieben wird. 225   (5) 1Bei der Abgabe der Bachelorarbeit haben die Studierenden schriftlich zu versichern, dass sie die Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung des KIT zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet haben. 2Wenn diese Erklärung nicht enthalten ist, wird die Arbeit nicht angenommen. 3Die Erklärung kann wie folgt', metadata={})]},\n",
       " {'query': 'wieviele leistungspunkte umfängt die bachelorarbeit?',\n",
       "  'result': ' 12 Leistungspunkte.',\n",
       "  'source_documents': [Document(page_content='nach wissenschaftlichen Methoden zu bearbeiten. 2Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten. 3Die maximale Bearbeitungsdauer beträgt vier Monate. 4Thema und Aufgabenstellung sind an den vorgesehenen Umfang anzupassen. 5Der Prüfungsausschuss legt fest, in welchen Sprachen die Bachelorarbeit geschrieben werden kann. 6Auf Antrag des Studierenden kann der/die Prüfende genehmigen, dass die Bachelorarbeit in einer anderen Sprache als Deutsch geschrieben wird. 225   (5) 1Bei der Abgabe der Bachelorarbeit haben die Studierenden schriftlich zu versichern, dass sie die Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung des KIT zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet haben. 2Wenn diese Erklärung nicht enthalten ist, wird die Arbeit nicht angenommen. 3Die Erklärung kann wie folgt', metadata={})]},\n",
       " {'query': 'wieviele leistungspunkten entspricht die bachelorarbeit?',\n",
       "  'result': ' 12 Leistungspunkten.',\n",
       "  'source_documents': [Document(page_content='nach wissenschaftlichen Methoden zu bearbeiten. 2Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten. 3Die maximale Bearbeitungsdauer beträgt vier Monate. 4Thema und Aufgabenstellung sind an den vorgesehenen Umfang anzupassen. 5Der Prüfungsausschuss legt fest, in welchen Sprachen die Bachelorarbeit geschrieben werden kann. 6Auf Antrag des Studierenden kann der/die Prüfende genehmigen, dass die Bachelorarbeit in einer anderen Sprache als Deutsch geschrieben wird. 225   (5) 1Bei der Abgabe der Bachelorarbeit haben die Studierenden schriftlich zu versichern, dass sie die Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung des KIT zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet haben. 2Wenn diese Erklärung nicht enthalten ist, wird die Arbeit nicht angenommen. 3Die Erklärung kann wie folgt', metadata={})]},\n",
       " {'query': \"Was ist der Umfang der bachelorarbeit, bzw. wieviele leistungspunkte umfasst die bachelorarbeit? welchen umfang hat das modul 'bachelorarbeit'?\",\n",
       "  'result': ' Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten.',\n",
       "  'source_documents': [Document(page_content='nach wissenschaftlichen Methoden zu bearbeiten. 2Der Umfang der Bachelorarbeit entspricht 12 Leistungspunkten. 3Die maximale Bearbeitungsdauer beträgt vier Monate. 4Thema und Aufgabenstellung sind an den vorgesehenen Umfang anzupassen. 5Der Prüfungsausschuss legt fest, in welchen Sprachen die Bachelorarbeit geschrieben werden kann. 6Auf Antrag des Studierenden kann der/die Prüfende genehmigen, dass die Bachelorarbeit in einer anderen Sprache als Deutsch geschrieben wird. 225   (5) 1Bei der Abgabe der Bachelorarbeit haben die Studierenden schriftlich zu versichern, dass sie die Arbeit selbstständig verfasst und keine anderen als die angegebenen Quellen und Hilfsmittel benutzt haben, die wörtlich oder inhaltlich übernommenen Stellen als solche kenntlich gemacht und die Satzung des KIT zur Sicherung guter wissenschaftlicher Praxis in der jeweils gültigen Fassung beachtet haben. 2Wenn diese Erklärung nicht enthalten ist, wird die Arbeit nicht angenommen. 3Die Erklärung kann wie folgt', metadata={})]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'apply' method to get all results in a list\n",
    "retrieval_chain.apply(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in prompts:\n",
    "    result = retrieval_chain({\"query\": query})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d0465bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'In wiefern werden ein Diploma Supplement und ein Zeugnis und ein Transcript of Records ausgestellt?',\n",
       " 'result': \" Alle drei Dokumente werden ausgestellt. Das Zeugnis enthält die Gesamtnote und die Leistungspunkte, das Diploma Supplement entspricht den Vorgaben des ECTS Users' Guide und das Transcript of Records enthält alle erbrachten Studien- und Prüfungsleistungen.\",\n",
       " 'source_documents': [Document(page_content='Leistungspunkte und die Gesamtnote. 2Sofern gemäß § 7 Abs. 2 Satz 2 eine differenzierte Bewertung einzelner Prüfungsleistungen vorgenommen wurde, wird auf dem Zeugnis auch die entsprechende Dezimalnote ausgewiesen; § 7 Abs. 4 bleibt unberührt. 3Das Zeugnis ist von der KIT-Dekanin/dem KIT-Dekan der KIT-Fakultät und von der/dem Vorsitzenden des Prü-fungsausschusses zu unterzeichnen. (3) 1Mit dem Zeugnis erhalten die Studierenden ein Diploma Supplement in deutscher und englischer Sprache, das den Vorgaben des jeweils gültigen ECTS Users‘ Guide entspricht, sowie ein Transcript of Records in deutscher und englischer Sprache.  (4) 1Das Transcript of Records enthält in strukturierter Form alle erbrachten Studien- und Prü-fungsleistungen. 2Dies beinhaltet alle Fächer und Fachnoten samt den zugeordneten Leistungspunkten, die dem jeweiligen Fach zugeordneten Module mit den Modulnoten und zugeordneten Leistungspunkten sowie die den Modulen zugeordneten Erfolgskontrollen samt Noten und', metadata={})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"In wiefern werden ein Diploma Supplement und ein Zeugnis und ein Transcript of Records ausgestellt?\"\n",
    "retrieval_chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b374f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''In wiefern erfüllt der Studiengang folgendes Kriterium: \n",
    "\n",
    "(1) 1 Im System gestufter Studiengänge ist der Bachelorabschluss der erste berufsqualifizierende Regelabschluss eines Hochschulstudiums; der Masterabschluss stellt einen weiteren berufsqualifizierenden Hochschulabschluss dar. 2Grundständige Studiengänge, die unmittelbar zu einem Masterabschluss führen, sind mit Ausnahme der in Absatz 3 genannten Studiengänge ausgeschlossen. (2) 1Die Regelstudienzeiten für ein Vollzeitstudium betragen sechs, sieben oder acht Semester bei den Bachelorstudiengängen und vier, drei oder zwei Semester bei den Masterstudiengängen. 2 Im Bachelorstudium beträgt die Regelstudienzeit im Vollzeitstudium mindestens drei Jahre. 3Bei konsekutiven Studiengängen beträgt die Gesamtregelstudienzeit im Vollzeitstudium fünf Jahre (zehn Semester). 4Wenn das Landesrecht dies vorsieht, sind kürzere und längere Regelstudienzeiten bei entsprechender studienorganisatorischer - 3 Gestaltung ausnahmsweise möglich, um den Studierenden eine individuelle Lernbiografie, insbesondere durch Teilzeit-, Fern-, berufsbegleitendes oder duales Studium sowie berufspraktische Semester, zu ermöglichen. 5Abweichend von Satz 3 können in den künstlerischen Kernfächern an Kunstund Musikhochschulen nach näherer Bestimmung des Landesrechts konsekutive Bachelor- und Masterstudiengänge auch mit einer Gesamtregelstudienzeit von sechs Jahren eingerichtet werden. (3) Theologische Studiengänge, die für das Pfarramt, das Priesteramt und den Beruf der Pastoralreferentin oder des Pastoralreferenten qualifizieren („Theologisches Vollstudium“), müssen nicht gestuft sein und können eine Regelstudienzeit von zehn Semestern aufweisen.\n",
    "'''\n",
    "retrieval_chain({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da7e5a",
   "metadata": {},
   "source": [
    "### HyDE with multi-document generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf8d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_llm = OpenAI(n=2, best_of=2)\n",
    "\n",
    "embeddings_multi = HypotheticalDocumentEmbedder.from_llm(\n",
    "    multi_llm, base_embeddings, \"web_search\"\n",
    ")\n",
    "\n",
    "vectorstore_multi = FAISS.from_texts(text_chunks, embedding= embeddings_multi) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f20f967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create retrieval chain with HyDE vectorstore\n",
    "retrieval_chain_hyde_m = RetrievalQA.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = vectorstore_multi.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'was ist der umfang der bachelorarbeit, bzw. wieviele leistungspunkte umfasst die bachelorarbeit?'\n",
    "retrieval_chain_hyde_m({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6fcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bee06d70",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- HyDE seems to work much better - much more flexibility in formulating questions. 3 / 3 correct answers instead of 1/3\n",
    "- but also much slower, probably about 10-15x slower. Up to 1 minute to answer 3 questions\n",
    "- does not perform well when mixing questions which might be covered in multiple docs\n",
    "\n",
    "#### To-Dos\n",
    "- Use other vector stores? Chroma??\n",
    "- Unit tests (save for PyCharm)\n",
    "- Look at HyDE docs - appears doc is submitted instead of user query, Can I see what kind of document is generated? HyDE can perhaps be used for a chatbot application (see docs: https://github.com/hwchase17/langchain/blob/8502117f62fc4caa53d504ccc9d4e6a512006e7f/langchain/chains/hyde/base.py#L20)\n",
    "- Need to setup QA: proportion of chatbot answers that are correct?\n",
    "- Setup columns in Streamlit which display docs and proposed answers\n",
    "- Annotation - retrieval of relevant string snippets?\n",
    "- Extracting entities? https://python.langchain.com/docs/modules/chains/additional/extraction\n",
    "- Add Metadata to docs: https://python.langchain.com/docs/modules/chains/popular/vector_db_qa#return-source-documents:~:text=Return%20Source%20Documents\n",
    "- Adapt to cheapest LLM model (ChatOpenAI model_name = gpt-3.5-turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9b121c",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d41b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question):\n",
    "    prompt_template_de = f'''Du bist ein Audit-Assistent für Qualitätssicherung im Hochschulwesen. \n",
    "    Deine Aufgabe ist es, Fragen über die Inhalte von Hochschulunterlagen wahrheitsgemäß zu beantworten. \n",
    "    Wenn die Unterlagen über keine Inhalte verfügen, die für die gestellte Frage relevant sind, teilst du dies mit. \n",
    "    Dein Ton ist formal und professionell. \n",
    "    \n",
    "    Frage: \"Welche Informationen gibt es bezüglich der Vergabe eines Diploma Supplements?\"\n",
    "    Antwort: \"Gemäß der Prüfungsordnung erhalten die Studierenden beim Abschluss des Studiums ein Diploma Supplement auf Deutsch und Englisch wie auch ein Transcript of Records\"\n",
    "\n",
    "    Frage: \"In wiefern verfügen die Studiengänge über Kooperationen mit nicht-hochschulischen Einrichtungen?\"\n",
    "    Antwort: \"Aus den zur Verfügung stehenden Unterlagen ist es nicht ersichtlich, in wiefern Kooperationen mit nicht-hochschulischen Einrichtungen bestehen.\n",
    "\n",
    "    Frage: \"Wenn es sich um einen Masterstudiengang handelt, ist er konsekutiv oder weiterbildend?\"\n",
    "    Antwort: \"Es handelt sich um einen Bachelorstudiengang, daher ist dieses Kriterium für diesen Studiengang nicht relevant.\"\n",
    "\n",
    "    Frage: {question}\n",
    "    Antwort: '''\n",
    "    \n",
    "    return prompt_template_de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c3e3e",
   "metadata": {},
   "source": [
    "### Setup loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b7f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path):\n",
    "    file = open(file_path, encoding=\"utf-8\")   # specifying encoding necessary to display German characters\n",
    "    criteria_sets = json.load(file)\n",
    "    return criteria_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
